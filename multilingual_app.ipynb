{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfDjQrw7oQ27e3bViXXQP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsikrishnakusetty555/Multilingual-Speech-Recognition-Model-for-RAG-Without-Training/blob/main/multilingual_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Modules"
      ],
      "metadata": {
        "id": "GrFPo7dOpRk4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ZwyGqgHIVG",
        "outputId": "42ba9553-b8ee-478e-b801-22f3a6e9a577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=fa7f26f3ee48fac93e004acaa0bcd5f024f3bc7829bd91217d2b90e021795b9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers sentencepiece -q\n"
      ],
      "metadata": {
        "id": "CAvfNhx2KgDp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "qL1qYlNTHUb0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9fwn6QpHjyH",
        "outputId": "2b739e49-0cc1-479e-814a-db3de6a5fdcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.611s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "iWTm2OAoHpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit App"
      ],
      "metadata": {
        "id": "usy27GhXHzZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import whisper\n",
        "from transformers import pipeline\n",
        "\n",
        "st.title(\"Multilingual Speech Recognition Model for RAG Without Training\")\n",
        "\n",
        "languages = [\n",
        "    \"Afrikaans (af)\", \"Amharic (am)\", \"Arabic (ar)\", \"Asturian (ast)\", \"Azerbaijani (az)\",\n",
        "    \"Bashkir (ba)\", \"Belarusian (be)\", \"Bulgarian (bg)\", \"Bengali (bn)\", \"Breton (br)\",\n",
        "    \"Bosnian (bs)\", \"Catalan; Valencian (ca)\", \"Cebuano (ceb)\", \"Czech (cs)\", \"Welsh (cy)\",\n",
        "    \"Danish (da)\", \"German (de)\", \"Greeek (el)\", \"English (en)\", \"Spanish (es)\", \"Estonian (et)\",\n",
        "    \"Persian (fa)\", \"Fulah (ff)\", \"Finnish (fi)\", \"French (fr)\", \"Western Frisian (fy)\",\n",
        "    \"Irish (ga)\", \"Gaelic; Scottish Gaelic (gd)\", \"Galician (gl)\", \"Gujarati (gu)\", \"Hausa (ha)\",\n",
        "    \"Hebrew (he)\", \"Hindi (hi)\", \"Croatian (hr)\", \"Haitian; Haitian Creole (ht)\", \"Hungarian (hu)\",\n",
        "    \"Armenian (hy)\", \"Indonesian (id)\", \"Igbo (ig)\", \"Iloko (ilo)\", \"Icelandic (is)\", \"Italian (it)\",\n",
        "    \"Japanese (ja)\", \"Javanese (jv)\", \"Georgian (ka)\", \"Kazakh (kk)\", \"Central Khmer (km)\",\n",
        "    \"Kannada (kn)\", \"Korean (ko)\", \"Luxembourgish; Letzeburgesch (lb)\", \"Ganda (lg)\", \"Lingala (ln)\",\n",
        "    \"Lao (lo)\", \"Lithuanian (lt)\", \"Latvian (lv)\", \"Malagasy (mg)\", \"Macedonian (mk)\", \"Malayalam (ml)\",\n",
        "    \"Mongolian (mn)\", \"Marathi (mr)\", \"Malay (ms)\", \"Burmese (my)\", \"Nepali (ne)\", \"Dutch; Flemish (nl)\",\n",
        "    \"Norwegian (no)\", \"Northern Sotho (ns)\", \"Occitan (post 1500) (oc)\", \"Oriya (or)\", \"Panjabi; Punjabi (pa)\",\n",
        "    \"Polish (pl)\", \"Pushto; Pashto (ps)\", \"Portuguese (pt)\", \"Romanian; Moldavian; Moldovan (ro)\", \"Russian (ru)\",\n",
        "    \"Sindhi (sd)\", \"Sinhala; Sinhalese (si)\", \"Slovak (sk)\", \"Slovenian (sl)\", \"Somali (so)\", \"Albanian (sq)\",\n",
        "    \"Serbian (sr)\", \"Swati (ss)\", \"Sundanese (su)\", \"Swedish (sv)\", \"Swahili (sw)\", \"Tamil (ta)\", \"Thai (th)\",\n",
        "    \"Tagalog (tl)\", \"Tswana (tn)\", \"Turkish (tr)\", \"Ukrainian (uk)\", \"Urdu (ur)\", \"Uzbek (uz)\", \"Vietnamese (vi)\",\n",
        "    \"Wolof (wo)\", \"Xhosa (xh)\", \"Yiddish (yi)\", \"Yoruba (yo)\", \"Chinese (zh)\", \"Zulu (zu)\"\n",
        "]\n",
        "\n",
        "if \"original_text\" not in st.session_state:\n",
        "    st.session_state.original_text = \"\"\n",
        "\n",
        "audio_file = st.file_uploader(\"Upload Audio\", type=[\"mp3\", \"wav\", \"ogg\"])\n",
        "\n",
        "if audio_file is not None:\n",
        "    option = st.selectbox(\n",
        "        \"Choose the Model\",\n",
        "        (\"base\", \"medium\", \"large\"),\n",
        "        index=0,\n",
        "        help=\"Select Model...\"\n",
        "    )\n",
        "\n",
        "    btn_transcribe = st.button(\"Transcribe\")\n",
        "\n",
        "    if btn_transcribe:\n",
        "        with open(\"uploaded_audio_file.mp3\", \"wb\") as file:\n",
        "            file.write(audio_file.read())\n",
        "\n",
        "        model = whisper.load_model(option)\n",
        "        result = model.transcribe(\"uploaded_audio_file.mp3\")\n",
        "        st.session_state.original_text = result[\"text\"]\n",
        "\n",
        "        st.write(\"Transcription Result:\")\n",
        "        st.write(st.session_state.original_text)\n",
        "\n",
        "    target_lang = st.selectbox(\"Select target language:\", languages)\n",
        "    btn_translate = st.button(\"Translate\")\n",
        "\n",
        "    if btn_translate and st.session_state.original_text:  # Check if original_text is not empty\n",
        "        target_lang_code = target_lang[-3:-1]\n",
        "        pipe = pipeline(task='text2text-generation', model='facebook/m2m100_418M')\n",
        "        translation_result = pipe(st.session_state.original_text, forced_bos_token_id=pipe.tokenizer.get_lang_id(lang=target_lang_code))\n",
        "\n",
        "        st.write(\"Translation Result:\")\n",
        "        st.write(translation_result[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAAQT1m_XhvT",
        "outputId": "a8eb7508-5558-4dec-e591-25694e97a47c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Streamlit App"
      ],
      "metadata": {
        "id": "XDb2_O-ZH6qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir84e-nUH4Yl",
        "outputId": "49c4e11a-feb2-4fec-b34e-0f9bcc29b009"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.148.218.81"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBFHkoXlIE44",
        "outputId": "9691b439-42f9-4b58-82ac-bbae2dfa514a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.996s\n",
            "your url is: https://many-glasses-send.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}